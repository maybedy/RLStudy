## Project
- Bellemare, M. G., Naddaf, Y., Veness, J., & Bowling, M. (2013). The Arcade Learning Environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47, 253–279. doi:10.1613/jair.3912 [TA JP Hong on Oct 19]
## Large State and/or Action Spaces
- Lagoudakis, M. G., & Parr, R. (2003). Least-squares policy iteration. The Journal of Machine Learning Research, 4, 1107–1149. doi:10.1162/jmlr.2003.4.6.1107
- Taylor, G., & Parr, R. (2009). Kernelized value function approximation for reinforcement learning. In Proceedings of the 26th Annual International Conference on Machine Learning - ICML ’09 (pp. 1–8). New York, New York, USA: ACM Press. doi:10.1145/1553374.1553504
- Kveton, B., & Theocharous, G. (2012). Kernel-Based Reinforcement Learning on Representative States. In AAAI (pp. 977–983).
- Engel, Y., Mannor, S., & Meir, R. (2005). Reinforcement learning with Gaussian processes. In Proceedings of the 22nd international conference on Machine learning - ICML ’05 (pp. 201–208). New York, New York, USA: ACM Press. doi:10.1145/1102351.1102377
- Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. a, Veness, J., Bellemare, M. G., … Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. doi:10.1038/nature14236
- Grunewalder, S., Lever, G., Baldassarre, L., Pontil, M., & Gretton, A. (2012). Modelling transition dynamics in MDPs with RKHS embeddings. In Proceedings of the 29th International Conference on Machine Learning (ICML-12). Retrieved from http://arxiv.org/abs/1206.4655
- Hasselt, H. Van. (2010). Double Q-learning. In Proceedings of NIPS (pp. 1–9).
- Peters, J., Mülling, K., & Altun, Y. (2010). Relative Entropy Policy Search. In Proceedings of AAAI. Retrieved from http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1851/2264
## Rewards, or more generally, feedback
- Wiewiora, E. (2003). Potential-Based Shaping and Q-Value Initialization are Equivalent. Journal of Artificial Intelligence Research (JAIR), 19, 205–208.
- Ng, A. Y., Harada, D., & Russell, S. (1999). Policy Invariance under Reward Transformations - Theory and Application to Reward Shaping. In Proceedings of ICML (pp. 278–287). 
- Wiewiora, E., Cottrell, G., & Elkan, C. (2003). Principled methods for advising reinforcement learning agents. In Proceedings of ICML. 
- Harutyunyan, A., Devlin, S., Vrancx, P., & Nowé, A. (2015). Expressing Arbitrary Reward Functions as Potential-Based Advice. In AAAI. 
- Loftin, R., Peng, B., MacGlashan, J., Littman, M. L., Taylor, M. E., Huang, J., & Roberts, D. L. (2015). Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning. Autonomous Agents and Multi-Agent Systems. doi:10.1007/s10458-015-9283-7
## Bayesian Reinforcement Learning
- Dearden, R., Friedman, N., & Russell, S. (1998). Bayesian Q-learning. In Proceedings of the 15th National Conference on Artificial Intelligence (AAAI).
- Strens, M. (2000). A Bayesian Framework for Reinforcement Learning. In Proceedings of the 17th International Conference on Machine Learning (ICML).
- Poupart, P., Vlassis, N., Hoey, J., & Regan, K. (2006). An analytic solution to discrete Bayesian reinforcement learning. In Proceedings of the 23rd international conference on Machine learning - ICML ’06 (pp. 697–704). New York, New York, USA: ACM Press. doi:10.1145/1143844.1143932
- Ross, S., & Pineau, J. (2008). Model-based Bayesian reinforcement learning in large structured domains. In Uncertainty In Artificial Intelligence (UAI).
- Guez, A., Silver, D., & Dayan, P. (2012). Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search. In Advances in Neural Information Processing Systems (NIPS) (pp. 1–9).
## Theory
- Strehl, A. L., & Littman, M. L. (2008). An analysis of model-based Interval Estimation for Markov Decision Processes. Journal of Computer and System Sciences, 74(8), 1309–1331. doi:10.1016/j.jcss.2007.08.009
- Kolter, J. Z., & Ng, A. Y. (2009). Near-Bayesian exploration in polynomial time. In Proceedings of ICML (pp. 1–8). New York, New York, USA: ACM Press. doi:10.1145/1553374.1553441
## Inverse Reinforcement Learning and Apprenticeship Learning
- Ng, A. Y., & Russell, S. (2000). Algorithms for Inverse Reinforcement Learning. In Proceedings of the 17th International Conference on Machine Learning (ICML) (pp. 663–670). Standord, CA, USA.
- Abbeel, P., & Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. In R. Greiner & D. Schuurmans (Eds.), Proceedings of the 21st International Conference on Machine Learning (ICML) (pp. 1–8). Banff, Alberta, Canada: ACM. Retrieved from http://www.scopus.com/inward/record.url?eid=2-s2.0-14344251217&partnerID=40
- Ziebart, B. D., Maas, A. L., Bagnell, J. A., & Dey, A. K. (2008). Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd AAAI Conference on Artificial intelligence (pp. 1433–1438). Chicago, Illinois, USA: AAAI Press.
- Ramachandran, D., & Amir, E. (2007). Bayesian Inverse Reinforcement Learning. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI) (pp. 2586–2591). Hyderabad, India.
- Syed, U., & Schapire, R. E. (2007). A Game-Theoretic approach to apprenticeship learning. In Advances in Neural Information Processing Systems (NIPS) 20 (Vol. 20, pp. 1449–1456). Vancouver, British Columbia, Canada: MIT Press.
## Bandits
- Jaksch, T., Ortner, R., & Auer, P. (2010). Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11, 1563–1600.
- Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine Learning, 235–256. Retrieved from http://link.springer.com/article/10.1023/A:1013689704352
- Li, L., Chu, W., Langford, J., & Schapire, R. E. (2010). A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web (WWW) (pp. 661–670). New York, New York, USA: ACM Press. doi:10.1145/1772690.1772758
- Agrawal, S., & Goyal, N. (2012). Thompson sampling for contextual bandits with linear payoffs. In Proceedings of ICML (Vol. 28). Retrieved from http://arxiv.org/abs/1209.3352
- Guez, Arthur; Silver, David; Dayan, Peter (2014). Better Optimism By Bayes: Adaptive Planning with Rich Models. arXiv:1402.1958
