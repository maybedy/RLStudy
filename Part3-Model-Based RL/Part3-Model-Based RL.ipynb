{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from modelAny import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-01 00:33:21,586] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = 8 # hidden layer \n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99\n",
    "decay_rate = 0.99 \n",
    "resume = False\n",
    "\n",
    "model_bs = 3 # batch size\n",
    "real_bs = 3\n",
    "\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 hidden layer for policy network\n",
    "- input : observation(shape=[4])\n",
    "- output : probability(shape=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None, 4], name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations, W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1, W2)\n",
    "probability = tf.nn.sigmoid(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables() # W1, W2\n",
    "input_y = tf.placeholder(tf.float32, [None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32, name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad, W2Grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss & gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loglik = tf.log(input_y * (input_y - probability) + (1 - input_y) * (input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages)\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars)) # list of [gradient, vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 hidden layer for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mH = 256 # model layer size\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [None, 5]) # 4+ 1?\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None, 5], name= \"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]), name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state, W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]), name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M, W2M) + B2M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output layer for model(observation, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wO = tf.get_variable(\"wO\", shape=[mH, 4], initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "bO = tf.Variable(tf.zeros([4]), name=\"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]), name=\"bR\")\n",
    "bD = tf.Variable(tf.zeros([1]), name=\"bD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_observation = tf.matmul(layer2M, wO, name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M, wR, name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M, wD, name=\"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32, [None, 4], name=\"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32, [None, 1], name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32, [None, 1], name=\"true_done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_state = tf.concat(1, [predicted_observation, predicted_reward, predicted_done])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss calculating from true & predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "done_loss = tf.mul(predicted_done, true_done) + tf.mul(1-predicted_done, 1-true_done) #?\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "\n",
    "# import sys\n",
    "\n",
    "# if sys.version_info >= (3, 0):\n",
    "#     def xrange(*args, **kwargs):\n",
    "#         return iter(range(*args, **kwargs))\n",
    "    \n",
    "def discount_rewards(r):\n",
    "    discounted_r = np.zeros_like(r) # same shape\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]), [1,5]) # -1 for last one , 0 for erase first of (1 X 4)\n",
    "    myPredict = sess.run([predicted_state], feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:, 0:4]\n",
    "    observation[:, 0] = np.clip(observation[:,0],-2.4, 2.4)\n",
    "    observation[:, 2] = np.clip(observation[:,2], -0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:, 5], 0, 1)\n",
    "    if doneP > 0.1 or len(xs) >= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the policy and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs, drs, ys, ds = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drawFromModel = False # if true, will use model for observations\n",
    "trainTheModel = True # to train the model\n",
    "trainThePolicy = False # to train the policy\n",
    "switch_point = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### launch the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Pref: Episode 4.000000. Reward 14.000000. action : 0.000000. mean reward 14.000000.\n",
      "World Pref: Episode 7.000000. Reward 19.333333. action : 0.000000. mean reward 15.793333.\n",
      "World Pref: Episode 10.000000. Reward 21.666667. action : 0.000000. mean reward 17.802067.\n",
      "World Pref: Episode 13.000000. Reward 18.000000. action : 0.000000. mean reward 19.424046.\n",
      "World Pref: Episode 16.000000. Reward 10.666667. action : 0.000000. mean reward 20.296472.\n",
      "World Pref: Episode 19.000000. Reward 24.000000. action : 0.000000. mean reward 22.493507.\n",
      "World Pref: Episode 22.000000. Reward 13.000000. action : 0.000000. mean reward 23.568572.\n",
      "World Pref: Episode 25.000000. Reward 30.333333. action : 0.000000. mean reward 26.366220.\n",
      "World Pref: Episode 28.000000. Reward 24.000000. action : 1.000000. mean reward 28.502558.\n",
      "World Pref: Episode 31.000000. Reward 13.666667. action : 0.000000. mean reward 29.584199.\n",
      "World Pref: Episode 34.000000. Reward 18.000000. action : 0.000000. mean reward 31.088357.\n",
      "World Pref: Episode 37.000000. Reward 24.333333. action : 0.000000. mean reward 33.210807.\n",
      "World Pref: Episode 40.000000. Reward 21.666667. action : 1.000000. mean reward 35.045365.\n",
      "World Pref: Episode 43.000000. Reward 12.666667. action : 0.000000. mean reward 35.961578.\n",
      "World Pref: Episode 46.000000. Reward 25.000000. action : 0.000000. mean reward 38.101963.\n",
      "World Pref: Episode 49.000000. Reward 17.666667. action : 0.000000. mean reward 39.487610.\n",
      "World Pref: Episode 52.000000. Reward 19.000000. action : 0.000000. mean reward 40.992733.\n",
      "World Pref: Episode 55.000000. Reward 19.333333. action : 0.000000. mean reward 42.516139.\n",
      "World Pref: Episode 58.000000. Reward 17.333333. action : 0.000000. mean reward 43.824311.\n",
      "World Pref: Episode 61.000000. Reward 19.000000. action : 0.000000. mean reward 45.286068.\n",
      "World Pref: Episode 64.000000. Reward 21.333333. action : 0.000000. mean reward 46.966541.\n",
      "World Pref: Episode 67.000000. Reward 20.333333. action : 0.000000. mean reward 48.530209.\n",
      "World Pref: Episode 70.000000. Reward 30.666667. action : 0.000000. mean reward 51.111573.\n",
      "World Pref: Episode 73.000000. Reward 12.333333. action : 0.000000. mean reward 51.833791.\n",
      "World Pref: Episode 76.000000. Reward 17.000000. action : 1.000000. mean reward 53.015453.\n",
      "World Pref: Episode 79.000000. Reward 16.333333. action : 0.000000. mean reward 54.118632.\n",
      "World Pref: Episode 82.000000. Reward 18.666667. action : 1.000000. mean reward 55.444112.\n",
      "World Pref: Episode 85.000000. Reward 25.333333. action : 0.000000. mean reward 57.423004.\n",
      "World Pref: Episode 88.000000. Reward 20.666667. action : 0.000000. mean reward 58.915441.\n",
      "World Pref: Episode 91.000000. Reward 20.000000. action : 0.000000. mean reward 60.326287.\n",
      "World Pref: Episode 94.000000. Reward 24.000000. action : 1.000000. mean reward 62.123024.\n",
      "World Pref: Episode 97.000000. Reward 13.000000. action : 0.000000. mean reward 62.801794.\n",
      "World Pref: Episode 100.000000. Reward 28.666667. action : 0.000000. mean reward 65.040442.\n",
      "World Pref: Episode 103.000000. Reward 20.666667. action : 1.000000. mean reward 66.456705.\n",
      "World Pref: Episode 106.000000. Reward 12.333333. action : 0.000000. mean reward 66.761330.\n",
      "World Pref: Episode 109.000000. Reward 19.000000. action : 1.000000. mean reward 99.911064.\n",
      "World Pref: Episode 112.000000. Reward 12.333333. action : 1.000000. mean reward 99.901939.\n",
      "World Pref: Episode 115.000000. Reward 28.000000. action : 1.000000. mean reward 101.931671.\n",
      "World Pref: Episode 118.000000. Reward 24.000000. action : 0.000000. mean reward 103.089989.\n",
      "World Pref: Episode 121.000000. Reward 29.333333. action : 1.000000. mean reward 104.791016.\n",
      "World Pref: Episode 124.000000. Reward 22.666667. action : 0.000000. mean reward 105.902351.\n",
      "World Pref: Episode 127.000000. Reward 13.333333. action : 0.000000. mean reward 106.808800.\n",
      "World Pref: Episode 130.000000. Reward 13.666667. action : 0.000000. mean reward 108.148506.\n",
      "World Pref: Episode 133.000000. Reward 13.333333. action : 0.000000. mean reward 108.347893.\n",
      "World Pref: Episode 136.000000. Reward 13.666667. action : 0.000000. mean reward 108.693459.\n",
      "World Pref: Episode 139.000000. Reward 22.333333. action : 0.000000. mean reward 110.497559.\n",
      "World Pref: Episode 142.000000. Reward 11.000000. action : 0.000000. mean reward 110.772697.\n",
      "World Pref: Episode 145.000000. Reward 17.333333. action : 0.000000. mean reward 121.534889.\n",
      "World Pref: Episode 148.000000. Reward 32.333333. action : 0.000000. mean reward 123.274536.\n",
      "World Pref: Episode 151.000000. Reward 14.000000. action : 0.000000. mean reward 125.109596.\n",
      "World Pref: Episode 154.000000. Reward 12.333333. action : 1.000000. mean reward 125.196411.\n",
      "World Pref: Episode 157.000000. Reward 22.666667. action : 0.000000. mean reward 125.906914.\n",
      "World Pref: Episode 160.000000. Reward 18.666667. action : 0.000000. mean reward 126.921059.\n",
      "World Pref: Episode 163.000000. Reward 27.000000. action : 0.000000. mean reward 129.875504.\n",
      "World Pref: Episode 166.000000. Reward 31.000000. action : 0.000000. mean reward 131.216278.\n",
      "World Pref: Episode 169.000000. Reward 24.000000. action : 0.000000. mean reward 132.153900.\n",
      "World Pref: Episode 172.000000. Reward 27.000000. action : 0.000000. mean reward 134.357422.\n",
      "World Pref: Episode 175.000000. Reward 21.666667. action : 0.000000. mean reward 134.590164.\n",
      "World Pref: Episode 178.000000. Reward 20.000000. action : 0.000000. mean reward 135.440002.\n",
      "World Pref: Episode 181.000000. Reward 22.000000. action : 0.000000. mean reward 151.425766.\n",
      "World Pref: Episode 184.000000. Reward 36.333333. action : 0.000000. mean reward 172.719467.\n",
      "World Pref: Episode 187.000000. Reward 17.000000. action : 1.000000. mean reward 173.101379.\n",
      "World Pref: Episode 190.000000. Reward 29.000000. action : 1.000000. mean reward 173.532959.\n",
      "World Pref: Episode 193.000000. Reward 26.000000. action : 0.000000. mean reward 173.629440.\n",
      "World Pref: Episode 196.000000. Reward 29.000000. action : 0.000000. mean reward 174.710388.\n",
      "World Pref: Episode 199.000000. Reward 29.333333. action : 0.000000. mean reward 174.972305.\n",
      "World Pref: Episode 202.000000. Reward 35.333333. action : 0.000000. mean reward 208.286118.\n",
      "World Pref: Episode 205.000000. Reward 17.666667. action : 0.000000. mean reward 228.245605.\n",
      "World Pref: Episode 208.000000. Reward 25.666667. action : 0.000000. mean reward 229.595596.\n",
      "World Pref: Episode 211.000000. Reward 19.000000. action : 0.000000. mean reward 235.776749.\n",
      "World Pref: Episode 214.000000. Reward 19.333333. action : 1.000000. mean reward 233.686600.\n",
      "World Pref: Episode 217.000000. Reward 18.000000. action : 1.000000. mean reward 241.283386.\n",
      "World Pref: Episode 220.000000. Reward 29.666667. action : 0.000000. mean reward 240.732437.\n",
      "World Pref: Episode 223.000000. Reward 26.333333. action : 0.000000. mean reward 239.826843.\n",
      "World Pref: Episode 226.000000. Reward 33.666667. action : 1.000000. mean reward 239.635620.\n",
      "World Pref: Episode 229.000000. Reward 23.000000. action : 0.000000. mean reward 240.177734.\n",
      "World Pref: Episode 232.000000. Reward 21.666667. action : 1.000000. mean reward 238.483200.\n",
      "World Pref: Episode 235.000000. Reward 41.666667. action : 0.000000. mean reward 245.551819.\n",
      "World Pref: Episode 238.000000. Reward 18.666667. action : 0.000000. mean reward 244.306534.\n",
      "World Pref: Episode 241.000000. Reward 20.666667. action : 1.000000. mean reward 242.035843.\n",
      "World Pref: Episode 244.000000. Reward 23.000000. action : 0.000000. mean reward 260.444550.\n",
      "World Pref: Episode 247.000000. Reward 18.000000. action : 0.000000. mean reward 257.566620.\n",
      "World Pref: Episode 250.000000. Reward 27.666667. action : 1.000000. mean reward 256.547211.\n",
      "World Pref: Episode 253.000000. Reward 33.666667. action : 0.000000. mean reward 255.548508.\n",
      "World Pref: Episode 256.000000. Reward 16.666667. action : 0.000000. mean reward 254.062332.\n",
      "World Pref: Episode 259.000000. Reward 18.000000. action : 1.000000. mean reward 254.512802.\n",
      "World Pref: Episode 262.000000. Reward 22.666667. action : 0.000000. mean reward 272.376373.\n",
      "World Pref: Episode 265.000000. Reward 21.333333. action : 1.000000. mean reward 275.492889.\n",
      "World Pref: Episode 268.000000. Reward 33.333333. action : 1.000000. mean reward 273.916321.\n",
      "World Pref: Episode 271.000000. Reward 27.666667. action : 1.000000. mean reward 276.047089.\n",
      "World Pref: Episode 274.000000. Reward 35.000000. action : 0.000000. mean reward 276.154907.\n",
      "World Pref: Episode 277.000000. Reward 27.000000. action : 1.000000. mean reward 273.729767.\n",
      "World Pref: Episode 280.000000. Reward 26.666667. action : 1.000000. mean reward 271.748901.\n",
      "World Pref: Episode 283.000000. Reward 24.000000. action : 0.000000. mean reward 270.503876.\n",
      "World Pref: Episode 286.000000. Reward 49.666667. action : 0.000000. mean reward 271.681946.\n",
      "World Pref: Episode 289.000000. Reward 24.666667. action : 1.000000. mean reward 285.409149.\n",
      "World Pref: Episode 292.000000. Reward 44.333333. action : 1.000000. mean reward 285.291107.\n",
      "World Pref: Episode 295.000000. Reward 26.000000. action : 1.000000. mean reward 309.385895.\n",
      "World Pref: Episode 298.000000. Reward 35.000000. action : 0.000000. mean reward 307.866302.\n",
      "World Pref: Episode 301.000000. Reward 25.666667. action : 0.000000. mean reward 319.424194.\n",
      "World Pref: Episode 304.000000. Reward 21.333333. action : 1.000000. mean reward 347.397980.\n",
      "World Pref: Episode 307.000000. Reward 23.333333. action : 0.000000. mean reward 344.816315.\n",
      "World Pref: Episode 310.000000. Reward 31.333333. action : 1.000000. mean reward 344.162140.\n",
      "World Pref: Episode 313.000000. Reward 23.666667. action : 0.000000. mean reward 342.836548.\n",
      "World Pref: Episode 316.000000. Reward 22.333333. action : 1.000000. mean reward 338.962311.\n",
      "World Pref: Episode 319.000000. Reward 18.000000. action : 1.000000. mean reward 334.592072.\n",
      "World Pref: Episode 322.000000. Reward 43.666667. action : 1.000000. mean reward 333.177399.\n",
      "World Pref: Episode 325.000000. Reward 15.666667. action : 0.000000. mean reward 329.298859.\n",
      "World Pref: Episode 328.000000. Reward 31.666667. action : 0.000000. mean reward 333.112457.\n",
      "World Pref: Episode 331.000000. Reward 35.000000. action : 0.000000. mean reward 332.864532.\n",
      "World Pref: Episode 334.000000. Reward 24.333333. action : 1.000000. mean reward 329.287109.\n",
      "World Pref: Episode 337.000000. Reward 37.666667. action : 1.000000. mean reward 331.612518.\n",
      "World Pref: Episode 340.000000. Reward 27.000000. action : 0.000000. mean reward 328.659027.\n",
      "World Pref: Episode 343.000000. Reward 33.000000. action : 1.000000. mean reward 327.187653.\n",
      "World Pref: Episode 346.000000. Reward 27.666667. action : 0.000000. mean reward 325.313080.\n",
      "World Pref: Episode 349.000000. Reward 27.333333. action : 0.000000. mean reward 322.826569.\n",
      "World Pref: Episode 352.000000. Reward 18.000000. action : 1.000000. mean reward 319.228638.\n",
      "World Pref: Episode 355.000000. Reward 27.333333. action : 0.000000. mean reward 318.142334.\n",
      "World Pref: Episode 358.000000. Reward 36.333333. action : 1.000000. mean reward 316.293121.\n",
      "World Pref: Episode 361.000000. Reward 29.666667. action : 1.000000. mean reward 326.093994.\n",
      "World Pref: Episode 364.000000. Reward 67.333333. action : 0.000000. mean reward 339.629700.\n",
      "World Pref: Episode 367.000000. Reward 21.000000. action : 1.000000. mean reward 339.667206.\n",
      "World Pref: Episode 370.000000. Reward 31.333333. action : 1.000000. mean reward 336.758942.\n",
      "World Pref: Episode 373.000000. Reward 24.666667. action : 0.000000. mean reward 333.146881.\n",
      "World Pref: Episode 376.000000. Reward 35.000000. action : 0.000000. mean reward 335.450348.\n",
      "World Pref: Episode 379.000000. Reward 38.333333. action : 1.000000. mean reward 337.063293.\n",
      "World Pref: Episode 382.000000. Reward 37.333333. action : 0.000000. mean reward 337.097626.\n",
      "World Pref: Episode 385.000000. Reward 40.000000. action : 1.000000. mean reward 345.097626.\n",
      "World Pref: Episode 388.000000. Reward 28.000000. action : 0.000000. mean reward 369.212250.\n",
      "World Pref: Episode 391.000000. Reward 17.666667. action : 0.000000. mean reward 382.996063.\n",
      "World Pref: Episode 394.000000. Reward 25.666667. action : 0.000000. mean reward 384.337067.\n",
      "World Pref: Episode 397.000000. Reward 45.333333. action : 1.000000. mean reward 398.725250.\n",
      "World Pref: Episode 400.000000. Reward 46.333333. action : 1.000000. mean reward 402.766754.\n",
      "World Pref: Episode 403.000000. Reward 27.333333. action : 0.000000. mean reward 399.191315.\n",
      "World Pref: Episode 406.000000. Reward 19.000000. action : 1.000000. mean reward 417.795441.\n",
      "World Pref: Episode 409.000000. Reward 41.333333. action : 0.000000. mean reward 426.897705.\n",
      "World Pref: Episode 412.000000. Reward 26.333333. action : 0.000000. mean reward 424.717041.\n",
      "World Pref: Episode 415.000000. Reward 18.666667. action : 0.000000. mean reward 471.074615.\n",
      "World Pref: Episode 418.000000. Reward 12.666667. action : 0.000000. mean reward 516.019104.\n",
      "World Pref: Episode 421.000000. Reward 29.000000. action : 0.000000. mean reward 511.368042.\n",
      "World Pref: Episode 424.000000. Reward 35.000000. action : 0.000000. mean reward 506.546783.\n",
      "World Pref: Episode 427.000000. Reward 34.000000. action : 0.000000. mean reward 501.268341.\n",
      "World Pref: Episode 430.000000. Reward 16.000000. action : 1.000000. mean reward 495.886475.\n",
      "World Pref: Episode 433.000000. Reward 48.666667. action : 1.000000. mean reward 491.519867.\n",
      "World Pref: Episode 436.000000. Reward 39.000000. action : 1.000000. mean reward 488.359283.\n",
      "World Pref: Episode 439.000000. Reward 58.333333. action : 0.000000. mean reward 488.315582.\n",
      "World Pref: Episode 442.000000. Reward 35.333333. action : 1.000000. mean reward 487.038361.\n",
      "World Pref: Episode 445.000000. Reward 24.333333. action : 0.000000. mean reward 482.166748.\n",
      "World Pref: Episode 448.000000. Reward 29.333333. action : 0.000000. mean reward 477.657440.\n",
      "World Pref: Episode 451.000000. Reward 29.666667. action : 1.000000. mean reward 481.495117.\n",
      "World Pref: Episode 454.000000. Reward 52.666667. action : 1.000000. mean reward 480.415131.\n",
      "World Pref: Episode 457.000000. Reward 28.666667. action : 1.000000. mean reward 481.925201.\n",
      "World Pref: Episode 460.000000. Reward 41.666667. action : 1.000000. mean reward 480.879181.\n",
      "World Pref: Episode 463.000000. Reward 30.666667. action : 0.000000. mean reward 479.370880.\n",
      "World Pref: Episode 466.000000. Reward 81.666667. action : 1.000000. mean reward 480.459137.\n",
      "World Pref: Episode 469.000000. Reward 22.666667. action : 0.000000. mean reward 474.926758.\n",
      "World Pref: Episode 472.000000. Reward 31.666667. action : 1.000000. mean reward 470.561249.\n",
      "World Pref: Episode 475.000000. Reward 28.000000. action : 1.000000. mean reward 482.618011.\n",
      "World Pref: Episode 478.000000. Reward 56.000000. action : 1.000000. mean reward 492.827362.\n",
      "World Pref: Episode 481.000000. Reward 59.000000. action : 0.000000. mean reward 506.354004.\n",
      "World Pref: Episode 484.000000. Reward 30.666667. action : 1.000000. mean reward 502.250885.\n",
      "World Pref: Episode 487.000000. Reward 36.000000. action : 0.000000. mean reward 497.243652.\n",
      "World Pref: Episode 490.000000. Reward 35.666667. action : 1.000000. mean reward 491.388885.\n",
      "World Pref: Episode 493.000000. Reward 58.666667. action : 1.000000. mean reward 488.708374.\n",
      "World Pref: Episode 496.000000. Reward 66.333333. action : 1.000000. mean reward 487.529938.\n",
      "World Pref: Episode 499.000000. Reward 32.333333. action : 1.000000. mean reward 483.243927.\n",
      "World Pref: Episode 502.000000. Reward 27.666667. action : 1.000000. mean reward 506.996613.\n",
      "World Pref: Episode 505.000000. Reward 25.333333. action : 1.000000. mean reward 500.488434.\n",
      "World Pref: Episode 508.000000. Reward 72.333333. action : 1.000000. mean reward 500.815826.\n",
      "World Pref: Episode 511.000000. Reward 34.666667. action : 1.000000. mean reward 521.853088.\n",
      "World Pref: Episode 514.000000. Reward 36.333333. action : 1.000000. mean reward 518.772522.\n",
      "World Pref: Episode 517.000000. Reward 55.333333. action : 0.000000. mean reward 515.740723.\n",
      "World Pref: Episode 520.000000. Reward 58.333333. action : 1.000000. mean reward 512.663818.\n",
      "World Pref: Episode 523.000000. Reward 47.000000. action : 1.000000. mean reward 508.246857.\n",
      "World Pref: Episode 526.000000. Reward 68.000000. action : 1.000000. mean reward 517.810364.\n",
      "World Pref: Episode 529.000000. Reward 42.000000. action : 0.000000. mean reward 544.797546.\n",
      "World Pref: Episode 532.000000. Reward 38.000000. action : 1.000000. mean reward 547.427551.\n",
      "World Pref: Episode 535.000000. Reward 58.000000. action : 1.000000. mean reward 566.620544.\n",
      "World Pref: Episode 538.000000. Reward 55.666667. action : 1.000000. mean reward 562.573730.\n",
      "World Pref: Episode 541.000000. Reward 40.333333. action : 1.000000. mean reward 557.006775.\n",
      "World Pref: Episode 544.000000. Reward 37.000000. action : 1.000000. mean reward 552.466919.\n",
      "World Pref: Episode 547.000000. Reward 96.333333. action : 0.000000. mean reward 554.051086.\n",
      "World Pref: Episode 550.000000. Reward 28.333333. action : 1.000000. mean reward 548.312073.\n",
      "World Pref: Episode 553.000000. Reward 70.666667. action : 1.000000. mean reward 550.511292.\n",
      "World Pref: Episode 556.000000. Reward 52.666667. action : 0.000000. mean reward 546.370728.\n",
      "World Pref: Episode 559.000000. Reward 49.000000. action : 0.000000. mean reward 541.541504.\n",
      "World Pref: Episode 562.000000. Reward 41.666667. action : 0.000000. mean reward 536.578552.\n",
      "World Pref: Episode 565.000000. Reward 43.333333. action : 1.000000. mean reward 534.165527.\n",
      "World Pref: Episode 568.000000. Reward 37.000000. action : 1.000000. mean reward 531.196899.\n",
      "World Pref: Episode 571.000000. Reward 84.333333. action : 0.000000. mean reward 531.206238.\n",
      "World Pref: Episode 574.000000. Reward 49.000000. action : 1.000000. mean reward 526.686279.\n",
      "World Pref: Episode 577.000000. Reward 42.666667. action : 0.000000. mean reward 524.758728.\n",
      "World Pref: Episode 580.000000. Reward 39.000000. action : 1.000000. mean reward 523.837585.\n",
      "World Pref: Episode 583.000000. Reward 66.666667. action : 1.000000. mean reward 521.568848.\n",
      "World Pref: Episode 586.000000. Reward 75.000000. action : 0.000000. mean reward 519.787903.\n",
      "World Pref: Episode 589.000000. Reward 69.666667. action : 1.000000. mean reward 521.076599.\n",
      "World Pref: Episode 592.000000. Reward 46.000000. action : 1.000000. mean reward 520.396057.\n",
      "World Pref: Episode 595.000000. Reward 61.000000. action : 1.000000. mean reward 518.449402.\n",
      "World Pref: Episode 598.000000. Reward 53.333333. action : 1.000000. mean reward 525.800293.\n",
      "World Pref: Episode 601.000000. Reward 41.333333. action : 0.000000. mean reward 526.871948.\n",
      "World Pref: Episode 604.000000. Reward 56.666667. action : 1.000000. mean reward 528.197876.\n",
      "World Pref: Episode 607.000000. Reward 39.666667. action : 1.000000. mean reward 524.657654.\n",
      "World Pref: Episode 610.000000. Reward 49.333333. action : 1.000000. mean reward 522.163269.\n",
      "World Pref: Episode 613.000000. Reward 26.333333. action : 1.000000. mean reward 517.720276.\n",
      "World Pref: Episode 616.000000. Reward 48.000000. action : 1.000000. mean reward 541.024475.\n",
      "World Pref: Episode 619.000000. Reward 46.666667. action : 0.000000. mean reward 545.809753.\n",
      "World Pref: Episode 622.000000. Reward 39.333333. action : 1.000000. mean reward 569.825500.\n",
      "World Pref: Episode 625.000000. Reward 32.333333. action : 0.000000. mean reward 563.539856.\n",
      "World Pref: Episode 628.000000. Reward 51.666667. action : 1.000000. mean reward 558.540100.\n",
      "World Pref: Episode 631.000000. Reward 65.000000. action : 0.000000. mean reward 554.948608.\n",
      "World Pref: Episode 634.000000. Reward 71.333333. action : 0.000000. mean reward 552.612000.\n",
      "World Pref: Episode 637.000000. Reward 69.666667. action : 1.000000. mean reward 576.299622.\n",
      "World Pref: Episode 640.000000. Reward 66.333333. action : 1.000000. mean reward 574.757996.\n",
      "World Pref: Episode 643.000000. Reward 56.333333. action : 1.000000. mean reward 578.813354.\n",
      "World Pref: Episode 646.000000. Reward 71.666667. action : 1.000000. mean reward 591.434082.\n",
      "World Pref: Episode 649.000000. Reward 49.666667. action : 0.000000. mean reward 590.706360.\n",
      "World Pref: Episode 652.000000. Reward 78.000000. action : 1.000000. mean reward 601.772278.\n",
      "World Pref: Episode 655.000000. Reward 54.333333. action : 0.000000. mean reward 596.309143.\n",
      "World Pref: Episode 658.000000. Reward 44.666667. action : 0.000000. mean reward 594.414124.\n",
      "World Pref: Episode 661.000000. Reward 73.333333. action : 1.000000. mean reward 590.844177.\n",
      "World Pref: Episode 664.000000. Reward 46.666667. action : 1.000000. mean reward 584.368225.\n",
      "World Pref: Episode 667.000000. Reward 38.666667. action : 1.000000. mean reward 579.922485.\n",
      "World Pref: Episode 670.000000. Reward 64.000000. action : 0.000000. mean reward 577.032715.\n",
      "World Pref: Episode 673.000000. Reward 41.000000. action : 1.000000. mean reward 571.489685.\n",
      "World Pref: Episode 676.000000. Reward 121.666667. action : 0.000000. mean reward 608.374084.\n",
      "World Pref: Episode 679.000000. Reward 60.666667. action : 0.000000. mean reward 603.813843.\n",
      "World Pref: Episode 682.000000. Reward 63.333333. action : 0.000000. mean reward 600.184387.\n",
      "World Pref: Episode 685.000000. Reward 42.333333. action : 1.000000. mean reward 594.275574.\n",
      "World Pref: Episode 688.000000. Reward 42.666667. action : 0.000000. mean reward 588.027649.\n",
      "World Pref: Episode 691.000000. Reward 41.666667. action : 0.000000. mean reward 582.094788.\n",
      "World Pref: Episode 694.000000. Reward 30.333333. action : 0.000000. mean reward 576.537903.\n",
      "World Pref: Episode 697.000000. Reward 64.333333. action : 0.000000. mean reward 574.071594.\n",
      "World Pref: Episode 700.000000. Reward 54.333333. action : 0.000000. mean reward 569.028870.\n",
      "World Pref: Episode 703.000000. Reward 62.333333. action : 1.000000. mean reward 565.526123.\n",
      "World Pref: Episode 706.000000. Reward 105.000000. action : 0.000000. mean reward 566.707642.\n",
      "World Pref: Episode 709.000000. Reward 96.666667. action : 0.000000. mean reward 596.543213.\n",
      "World Pref: Episode 712.000000. Reward 48.000000. action : 0.000000. mean reward 706.760498.\n",
      "World Pref: Episode 715.000000. Reward 56.000000. action : 1.000000. mean reward 724.123291.\n",
      "World Pref: Episode 718.000000. Reward 54.333333. action : 0.000000. mean reward 716.025208.\n",
      "World Pref: Episode 721.000000. Reward 115.000000. action : 1.000000. mean reward 716.525146.\n",
      "World Pref: Episode 724.000000. Reward 75.000000. action : 1.000000. mean reward 710.235657.\n",
      "World Pref: Episode 727.000000. Reward 64.000000. action : 1.000000. mean reward 706.607239.\n",
      "World Pref: Episode 730.000000. Reward 61.666667. action : 0.000000. mean reward 699.722656.\n",
      "World Pref: Episode 733.000000. Reward 37.333333. action : 1.000000. mean reward 690.537537.\n",
      "World Pref: Episode 736.000000. Reward 68.333333. action : 0.000000. mean reward 693.169861.\n",
      "World Pref: Episode 739.000000. Reward 58.000000. action : 1.000000. mean reward 696.550842.\n",
      "World Pref: Episode 742.000000. Reward 77.666667. action : 0.000000. mean reward 695.169250.\n",
      "World Pref: Episode 745.000000. Reward 80.333333. action : 1.000000. mean reward 716.838562.\n",
      "World Pref: Episode 748.000000. Reward 54.000000. action : 0.000000. mean reward 709.711426.\n",
      "World Pref: Episode 751.000000. Reward 102.333333. action : 1.000000. mean reward 715.821838.\n",
      "World Pref: Episode 754.000000. Reward 64.666667. action : 1.000000. mean reward 709.197327.\n",
      "World Pref: Episode 757.000000. Reward 89.000000. action : 1.000000. mean reward 706.016907.\n",
      "World Pref: Episode 760.000000. Reward 51.000000. action : 0.000000. mean reward 707.182800.\n",
      "World Pref: Episode 763.000000. Reward 96.000000. action : 1.000000. mean reward 707.449036.\n",
      "World Pref: Episode 766.000000. Reward 84.333333. action : 1.000000. mean reward 709.768372.\n",
      "World Pref: Episode 769.000000. Reward 53.000000. action : 0.000000. mean reward 704.184631.\n",
      "World Pref: Episode 772.000000. Reward 90.000000. action : 1.000000. mean reward 700.363342.\n",
      "World Pref: Episode 775.000000. Reward 70.333333. action : 1.000000. mean reward 695.851013.\n",
      "World Pref: Episode 778.000000. Reward 92.333333. action : 0.000000. mean reward 694.317627.\n",
      "World Pref: Episode 781.000000. Reward 55.333333. action : 0.000000. mean reward 688.315125.\n",
      "World Pref: Episode 784.000000. Reward 93.666667. action : 1.000000. mean reward 686.737854.\n",
      "World Pref: Episode 787.000000. Reward 68.333333. action : 1.000000. mean reward 704.665039.\n",
      "World Pref: Episode 790.000000. Reward 30.666667. action : 1.000000. mean reward 699.224182.\n",
      "World Pref: Episode 793.000000. Reward 94.000000. action : 0.000000. mean reward 705.006042.\n",
      "World Pref: Episode 796.000000. Reward 89.666667. action : 1.000000. mean reward 701.468811.\n",
      "World Pref: Episode 799.000000. Reward 98.333333. action : 1.000000. mean reward 699.534180.\n",
      "World Pref: Episode 802.000000. Reward 79.000000. action : 1.000000. mean reward 694.448242.\n",
      "World Pref: Episode 805.000000. Reward 80.333333. action : 1.000000. mean reward 689.556885.\n",
      "World Pref: Episode 808.000000. Reward 83.000000. action : 0.000000. mean reward 733.287292.\n",
      "World Pref: Episode 811.000000. Reward 65.000000. action : 0.000000. mean reward 776.095032.\n",
      "World Pref: Episode 814.000000. Reward 105.333333. action : 0.000000. mean reward 773.562012.\n",
      "World Pref: Episode 817.000000. Reward 80.666667. action : 0.000000. mean reward 813.923767.\n",
      "World Pref: Episode 820.000000. Reward 63.666667. action : 1.000000. mean reward 831.996582.\n",
      "World Pref: Episode 823.000000. Reward 70.666667. action : 0.000000. mean reward 824.300781.\n",
      "World Pref: Episode 826.000000. Reward 71.000000. action : 1.000000. mean reward 836.195007.\n",
      "World Pref: Episode 829.000000. Reward 54.666667. action : 0.000000. mean reward 827.415100.\n",
      "World Pref: Episode 832.000000. Reward 68.333333. action : 1.000000. mean reward 821.695862.\n",
      "World Pref: Episode 835.000000. Reward 110.666667. action : 0.000000. mean reward 818.490051.\n",
      "World Pref: Episode 838.000000. Reward 46.000000. action : 1.000000. mean reward 808.621826.\n",
      "World Pref: Episode 841.000000. Reward 69.333333. action : 0.000000. mean reward 800.810852.\n",
      "World Pref: Episode 844.000000. Reward 72.666667. action : 0.000000. mean reward 795.777588.\n",
      "World Pref: Episode 847.000000. Reward 122.333333. action : 1.000000. mean reward 796.157654.\n",
      "World Pref: Episode 850.000000. Reward 73.000000. action : 0.000000. mean reward 816.982971.\n",
      "World Pref: Episode 853.000000. Reward 65.333333. action : 1.000000. mean reward 837.289368.\n",
      "World Pref: Episode 856.000000. Reward 69.333333. action : 1.000000. mean reward 833.497253.\n",
      "World Pref: Episode 859.000000. Reward 79.000000. action : 0.000000. mean reward 828.050232.\n",
      "World Pref: Episode 862.000000. Reward 90.000000. action : 1.000000. mean reward 822.795837.\n",
      "World Pref: Episode 865.000000. Reward 74.666667. action : 0.000000. mean reward 815.617859.\n",
      "World Pref: Episode 868.000000. Reward 73.000000. action : 0.000000. mean reward 811.008240.\n",
      "World Pref: Episode 871.000000. Reward 60.333333. action : 0.000000. mean reward 802.697571.\n",
      "World Pref: Episode 874.000000. Reward 108.666667. action : 0.000000. mean reward 800.364258.\n",
      "World Pref: Episode 877.000000. Reward 121.000000. action : 1.000000. mean reward 798.118896.\n",
      "World Pref: Episode 880.000000. Reward 79.333333. action : 0.000000. mean reward 795.550049.\n",
      "World Pref: Episode 883.000000. Reward 73.000000. action : 1.000000. mean reward 795.068359.\n",
      "World Pref: Episode 886.000000. Reward 62.333333. action : 0.000000. mean reward 791.241028.\n",
      "World Pref: Episode 889.000000. Reward 83.333333. action : 0.000000. mean reward 792.498291.\n",
      "World Pref: Episode 892.000000. Reward 106.333333. action : 0.000000. mean reward 792.414246.\n",
      "World Pref: Episode 895.000000. Reward 75.666667. action : 1.000000. mean reward 787.410645.\n",
      "World Pref: Episode 898.000000. Reward 67.000000. action : 1.000000. mean reward 783.052002.\n",
      "World Pref: Episode 901.000000. Reward 63.666667. action : 1.000000. mean reward 779.305847.\n",
      "World Pref: Episode 904.000000. Reward 72.000000. action : 1.000000. mean reward 774.650818.\n",
      "World Pref: Episode 907.000000. Reward 91.333333. action : 1.000000. mean reward 770.153381.\n",
      "World Pref: Episode 910.000000. Reward 115.000000. action : 0.000000. mean reward 784.027161.\n",
      "World Pref: Episode 913.000000. Reward 130.000000. action : 1.000000. mean reward 811.021729.\n",
      "World Pref: Episode 916.000000. Reward 85.666667. action : 0.000000. mean reward 809.089844.\n",
      "World Pref: Episode 919.000000. Reward 123.000000. action : 0.000000. mean reward 806.886169.\n",
      "World Pref: Episode 922.000000. Reward 91.666667. action : 1.000000. mean reward 801.269714.\n",
      "World Pref: Episode 925.000000. Reward 91.666667. action : 1.000000. mean reward 797.514404.\n",
      "World Pref: Episode 928.000000. Reward 69.000000. action : 1.000000. mean reward 791.162537.\n",
      "World Pref: Episode 931.000000. Reward 97.333333. action : 0.000000. mean reward 797.921082.\n",
      "World Pref: Episode 934.000000. Reward 93.333333. action : 1.000000. mean reward 794.803894.\n",
      "World Pref: Episode 937.000000. Reward 120.666667. action : 0.000000. mean reward 796.264893.\n",
      "World Pref: Episode 940.000000. Reward 122.000000. action : 1.000000. mean reward 822.177917.\n",
      "World Pref: Episode 943.000000. Reward 160.666667. action : 0.000000. mean reward 829.308350.\n",
      "World Pref: Episode 946.000000. Reward 116.666667. action : 1.000000. mean reward 828.617676.\n",
      "World Pref: Episode 949.000000. Reward 160.666667. action : 0.000000. mean reward 858.121765.\n",
      "World Pref: Episode 952.000000. Reward 140.333333. action : 1.000000. mean reward 862.938660.\n",
      "World Pref: Episode 955.000000. Reward 146.333333. action : 0.000000. mean reward 862.199463.\n",
      "World Pref: Episode 958.000000. Reward 123.000000. action : 0.000000. mean reward 858.721436.\n",
      "World Pref: Episode 961.000000. Reward 121.333333. action : 0.000000. mean reward 855.127014.\n",
      "World Pref: Episode 964.000000. Reward 136.333333. action : 0.000000. mean reward 854.009216.\n",
      "World Pref: Episode 967.000000. Reward 139.666667. action : 0.000000. mean reward 855.133240.\n",
      "World Pref: Episode 970.000000. Reward 71.000000. action : 1.000000. mean reward 873.366150.\n",
      "World Pref: Episode 973.000000. Reward 77.000000. action : 1.000000. mean reward 869.749084.\n",
      "World Pref: Episode 976.000000. Reward 190.666667. action : 1.000000. mean reward 874.251038.\n",
      "World Pref: Episode 979.000000. Reward 120.333333. action : 0.000000. mean reward 869.994568.\n",
      "World Pref: Episode 982.000000. Reward 121.666667. action : 0.000000. mean reward 866.103027.\n",
      "World Pref: Episode 985.000000. Reward 134.000000. action : 0.000000. mean reward 866.156982.\n",
      "World Pref: Episode 988.000000. Reward 138.666667. action : 1.000000. mean reward 866.105286.\n",
      "World Pref: Episode 991.000000. Reward 122.666667. action : 0.000000. mean reward 862.564026.\n",
      "World Pref: Episode 994.000000. Reward 160.333333. action : 0.000000. mean reward 867.687500.\n",
      "World Pref: Episode 997.000000. Reward 208.333333. action : 0.000000. mean reward 872.556396.\n",
      "997\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        if (reward_sum/batch_size > 150 and drawFromModel == False) or rendering== True:\n",
    "            env.render()\n",
    "            rendering = True\n",
    "        \n",
    "        x = np.reshape(observation, [1,4]) # 1 X 4 array\n",
    "        tfprob = sess.run(probability, feed_dict = {observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x)\n",
    "        y = 1 if action == 0 else 0\n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else: # observe from the learned model\n",
    "            observation, reward, done = stepModel(sess, xs, action)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) # record reward to call later \n",
    "        \n",
    "        if done:\n",
    "            if drawFromModel == False:\n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "            \n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs, drs, ys, ds = [], [], [], []\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1, :]\n",
    "                state_prevs = np.hstack([state_prevs,actions])\n",
    "                state_nexts = epx[1:, :]\n",
    "                rewards = np.array(epr[1:, :])\n",
    "                dones = np.array(epd[1:,:])\n",
    "                state_nextsAll = np.hstack([state_nexts, rewards, dones])\n",
    "                \n",
    "                feed_dict = {previous_state: state_prevs, true_observation: state_nexts, true_done:dones, true_reward:rewards}\n",
    "                loss,pState,_ = sess.run([model_loss, predicted_state, updateModel], feed_dict)\n",
    "            if trainThePolicy == True:\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads, feed_dict={observations: epx, input_y: epy, advantages:discounted_epr})\n",
    "                \n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix, grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number:\n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    sess.run(updateGrads, feed_dict={W1Grad: gradBuffer[0], W2Grad: gradBuffer[1]})\n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.1\n",
    "                if drawFromModel == False:\n",
    "                    # print(real_episodes,', ', reward_sum/real_bs,', ', action,', ', running_reward/real_bs, '\\n')\n",
    "                    print (\"World Pref: Episode %f. Reward %f. action : %f. mean reward %f.\" % (real_episodes, reward_sum/real_bs, action, running_reward/real_bs))\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "                \n",
    "                if episode_number > 100:\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "                \n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1, 0.1, [4])\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "print(real_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
