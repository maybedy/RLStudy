{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from modelAny import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-01 00:33:21,586] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H = 8 # hidden layer \n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99\n",
    "decay_rate = 0.99 \n",
    "resume = False\n",
    "\n",
    "model_bs = 3 # batch size\n",
    "real_bs = 3\n",
    "\n",
    "D = 4 # input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 hidden layer for policy network\n",
    "- input : observation(shape=[4])\n",
    "- output : probability(shape=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "observations = tf.placeholder(tf.float32, [None, 4], name=\"input_x\")\n",
    "W1 = tf.get_variable(\"W1\", shape=[4, H],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "layer1 = tf.nn.relu(tf.matmul(observations, W1))\n",
    "W2 = tf.get_variable(\"W2\", shape=[H, 1], \n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "score = tf.matmul(layer1, W2)\n",
    "probability = tf.nn.sigmoid(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables() # W1, W2\n",
    "input_y = tf.placeholder(tf.float32, [None,1], name=\"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name=\"reward_signal\")\n",
    "adam = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32, name=\"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32, name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad, W2Grad]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss & gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loglik = tf.log(input_y * (input_y - probability) + (1 - input_y) * (input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages)\n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars)) # list of [gradient, vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 hidden layer for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mH = 256 # model layer size\n",
    "\n",
    "input_data = tf.placeholder(tf.float32, [None, 5]) # 4+ 1?\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None, 5], name= \"previous_state\")\n",
    "W1M = tf.get_variable(\"W1M\", shape=[5, mH], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B1M = tf.Variable(tf.zeros([mH]), name=\"B1M\")\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state, W1M) + B1M)\n",
    "W2M = tf.get_variable(\"W2M\", shape=[mH, mH], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2M = tf.Variable(tf.zeros([mH]), name=\"B2M\")\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M, W2M) + B2M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output layer for model(observation, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wO = tf.get_variable(\"wO\", shape=[mH, 4], initializer=tf.contrib.layers.xavier_initializer())\n",
    "wR = tf.get_variable(\"wR\", shape=[mH, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "wD = tf.get_variable(\"wD\", shape=[mH, 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "bO = tf.Variable(tf.zeros([4]), name=\"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]), name=\"bR\")\n",
    "bD = tf.Variable(tf.zeros([1]), name=\"bD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_observation = tf.matmul(layer2M, wO, name=\"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M, wR, name=\"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M, wD, name=\"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32, [None, 4], name=\"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32, [None, 1], name=\"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32, [None, 1], name=\"true_done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_state = tf.concat(1, [predicted_observation, predicted_reward, predicted_done])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss calculating from true & predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "done_loss = tf.mul(predicted_done, true_done) + tf.mul(1-predicted_done, 1-true_done) #?\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelAdam = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "\n",
    "# import sys\n",
    "\n",
    "# if sys.version_info >= (3, 0):\n",
    "#     def xrange(*args, **kwargs):\n",
    "#         return iter(range(*args, **kwargs))\n",
    "    \n",
    "def discount_rewards(r):\n",
    "    discounted_r = np.zeros_like(r) # same shape\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def stepModel(sess, xs, action):\n",
    "    toFeed = np.reshape(np.hstack([xs[-1][0],np.array(action)]), [1,5]) # -1 for last one , 0 for erase first of (1 X 4)\n",
    "    myPredict = sess.run([predicted_state], feed_dict={previous_state: toFeed})\n",
    "    reward = myPredict[0][:,4]\n",
    "    observation = myPredict[0][:, 0:4]\n",
    "    observation[:, 0] = np.clip(observation[:,0],-2.4, 2.4)\n",
    "    observation[:, 2] = np.clip(observation[:,2], -0.4,0.4)\n",
    "    doneP = np.clip(myPredict[0][:, 5], 0, 1)\n",
    "    if doneP > 0.1 or len(xs) >= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "    return observation, reward, done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the policy and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs, drs, ys, ds = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drawFromModel = False # if true, will use model for observations\n",
    "trainTheModel = True # to train the model\n",
    "trainThePolicy = False # to train the policy\n",
    "switch_point = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### launch the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Pref: Episode 4.000000. Reward 25.000000. action : 0.000000. mean reward 25.000000.\n",
      "World Pref: Episode 7.000000. Reward 24.666667. action : 1.000000. mean reward 27.216667.\n",
      "World Pref: Episode 10.000000. Reward 14.666667. action : 1.000000. mean reward 28.411167.\n",
      "World Pref: Episode 13.000000. Reward 23.666667. action : 1.000000. mean reward 30.493722.\n",
      "World Pref: Episode 16.000000. Reward 15.000000. action : 1.000000. mean reward 31.688784.\n",
      "World Pref: Episode 19.000000. Reward 19.000000. action : 1.000000. mean reward 33.271897.\n",
      "World Pref: Episode 22.000000. Reward 26.333333. action : 0.000000. mean reward 35.572511.\n",
      "World Pref: Episode 25.000000. Reward 20.000000. action : 1.000000. mean reward 37.216786.\n",
      "World Pref: Episode 28.000000. Reward 23.333333. action : 0.000000. mean reward 39.177951.\n",
      "World Pref: Episode 31.000000. Reward 20.666667. action : 0.000000. mean reward 40.852838.\n",
      "World Pref: Episode 34.000000. Reward 22.333333. action : 0.000000. mean reward 42.677643.\n",
      "World Pref: Episode 37.000000. Reward 26.333333. action : 1.000000. mean reward 44.884200.\n",
      "World Pref: Episode 40.000000. Reward 28.000000. action : 0.000000. mean reward 47.235358.\n",
      "World Pref: Episode 43.000000. Reward 28.333333. action : 1.000000. mean reward 49.596338.\n",
      "World Pref: Episode 46.000000. Reward 20.333333. action : 1.000000. mean reward 51.133708.\n",
      "World Pref: Episode 49.000000. Reward 14.666667. action : 1.000000. mean reward 52.089038.\n",
      "World Pref: Episode 52.000000. Reward 28.333333. action : 1.000000. mean reward 54.401481.\n",
      "World Pref: Episode 55.000000. Reward 21.333333. action : 1.000000. mean reward 55.990799.\n",
      "World Pref: Episode 58.000000. Reward 18.000000. action : 1.000000. mean reward 57.230891.\n",
      "World Pref: Episode 61.000000. Reward 12.333333. action : 0.000000. mean reward 57.891916.\n",
      "World Pref: Episode 64.000000. Reward 26.333333. action : 0.000000. mean reward 59.946330.\n",
      "World Pref: Episode 67.000000. Reward 42.666667. action : 0.000000. mean reward 63.613533.\n",
      "World Pref: Episode 70.000000. Reward 35.333333. action : 0.000000. mean reward 66.510731.\n",
      "World Pref: Episode 73.000000. Reward 19.000000. action : 0.000000. mean reward 67.745624.\n",
      "World Pref: Episode 76.000000. Reward 21.333333. action : 0.000000. mean reward 69.201501.\n",
      "World Pref: Episode 79.000000. Reward 27.000000. action : 1.000000. mean reward 71.209486.\n",
      "World Pref: Episode 82.000000. Reward 19.333333. action : 1.000000. mean reward 72.430724.\n",
      "World Pref: Episode 85.000000. Reward 18.666667. action : 0.000000. mean reward 73.573084.\n",
      "World Pref: Episode 88.000000. Reward 20.333333. action : 0.000000. mean reward 74.870686.\n",
      "World Pref: Episode 91.000000. Reward 26.666667. action : 0.000000. mean reward 76.788646.\n",
      "World Pref: Episode 94.000000. Reward 21.000000. action : 1.000000. mean reward 78.120760.\n",
      "World Pref: Episode 97.000000. Reward 29.000000. action : 0.000000. mean reward 80.239552.\n",
      "World Pref: Episode 100.000000. Reward 28.333333. action : 0.000000. mean reward 82.270490.\n",
      "World Pref: Episode 103.000000. Reward 16.666667. action : 0.000000. mean reward 83.114452.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument to reversed() must be a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-7bf4413d5a38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdateModel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrainThePolicy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdiscounted_epr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscount_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mdiscounted_epr\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscounted_epr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mdiscounted_epr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscounted_epr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-621596f65d42>\u001b[0m in \u001b[0;36mdiscount_rewards\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdiscounted_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# same shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mrunning_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mrunning_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_add\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdiscounted_r\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_add\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument to reversed() must be a sequence"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        if (reward_sum/batch_size > 150 and drawFromModel == False) or rendering== True:\n",
    "            env.render()\n",
    "            rendering = True\n",
    "        \n",
    "        x = np.reshape(observation, [1,4]) # 1 X 4 array\n",
    "        tfprob = sess.run(probability, feed_dict = {observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "        # record various intermediates (needed later for backprop)\n",
    "        xs.append(x)\n",
    "        y = 1 if action == 0 else 0\n",
    "        ys.append(y)\n",
    "        \n",
    "        # step the model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else: # observe from the learned model\n",
    "            observation, reward, done = stepModel(sess, xs, action)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done*1)\n",
    "        drs.append(reward) # record reward to call later \n",
    "        \n",
    "        if done:\n",
    "            if drawFromModel == False:\n",
    "                real_episodes += 1\n",
    "            episode_number += 1\n",
    "            \n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            xs, drs, ys, ds = [], [], [], []\n",
    "            \n",
    "            if trainTheModel == True:\n",
    "                actions = np.array([np.abs(y-1) for y in epy][:-1])\n",
    "                state_prevs = epx[:-1, :]\n",
    "                state_prevs = np.hstack([state_prevs,actions])\n",
    "                state_nexts = epx[1:, :]\n",
    "                rewards = np.array(epr[1:, :])\n",
    "                dones = np.array(epd[1:,:])\n",
    "                state_nextsAll = np.hstack([state_nexts, rewards, dones])\n",
    "                \n",
    "                feed_dict = {previous_state: state_prevs, true_observation: state_nexts, true_done:dones, true_reward:rewards}\n",
    "                loss,pState,_ = sess.run([model_loss, predicted_state, updateModel], feed_dict)\n",
    "            if trainThePolicy == True:\n",
    "                discounted_epr = discount_rewards(epr).astype('float32')\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                tGrad = sess.run(newGrads, feed_dict={observations: epx, input_y: epy, advantages:discounted_epr})\n",
    "                \n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                for ix, grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number:\n",
    "                switch_point = episode_number\n",
    "                if trainThePolicy == True:\n",
    "                    sess.run(updateGrads, feed_dict={W1Grad: gradBuffer[0], W2Grad: gradBuffer[1]})\n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.1\n",
    "                if drawFromModel == False:\n",
    "                    # print(real_episodes,', ', reward_sum/real_bs,', ', action,', ', running_reward/real_bs, '\\n')\n",
    "                    print (\"World Pref: Episode %f. Reward %f. action : %f. mean reward %f.\" % (real_episodes, reward_sum/real_bs, action, running_reward/real_bs))\n",
    "                    if reward_sum/batch_size > 200:\n",
    "                        break\n",
    "                reward_sum = 0\n",
    "                \n",
    "                if episode_number > 100:\n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    trainThePolicy = not trainThePolicy\n",
    "                \n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1, 0.1, [4])\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "print(real_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
